{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks  method\n",
    "This tutorial shows how to gather training samples to train a Recurrent Neural Network (RNN) in order to perform a pixel-wise land-cover / land-use (LCLU) classification of AVIRIS scenes, within [Pycoal](https://github.com/capstone-coal/pycoal) library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Grab some training samples\n",
    "Firstly, we must pick an example AVIRIS scene to work on. We are going work with a clipped version of this AVIRIS-NG [ang20150420t182050](http://avirisng.jpl.nasa.gov/aviris_locator/y15_RGB/ang20150420t182050_RGB.jpeg) flight line, for sake of illustration. The clipped ENVI files are already included in ./data directory as can be seen in the output of the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with Neural Networks - and many other supervised machine learning techniques - it's necessary to gather an amount (maybe a lot) of training samples; that is, pixels samples, in case of Remote Sensing. The under consideration LCLU classes of such pixels, of course, must be known prior training phase, such that they can be used as input training samples for the learning algorithm to work. \n",
    "\n",
    "One way to get labeled samples is by performing a Spectral Angle Mapper (SAM) classification, the Pycoal's default algorithm. The aforementioned files were generated by SAM, with the [example_mineral.py](https://github.com/capstone-coal/pycoal/blob/master/examples/example_mineral.py) script, which created three versions of the clipped scene: a RGB image, derived from the orginal hyperspectral image; a classified image, in which each pixel's color denotes a distint LCLU class (e.g. red representing Schwertmannite cover); and, a \"scoremap\" in which each pixel holds the confidence value yielded by SAM classification for the pixel's assigned class (the lower the score, the darker the pixel).\n",
    "![Regio of Interest](clipped.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The challenge\n",
    "Now, the challenge is, given the sample space, choose the best subset of pixels that better suit the requirements of the learning algorithm. It is well known that a properly balanced set of training samples increases the overall model's accuracy. In other words, if we carefully pick the right set of samples used to train the model, it will be able to better generalize predictions over new unlabeled pixels. For example, it would be nice if - given a subset of pixels of which we know with confidence that belongs to the Schwertmannite class - we could predict every other unkown Schwertmannite pixels of the scene. The same procedure can be applied to any other LCLU class and this is what we are able to do through the use of the RNNs models included into Pycoal library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Hands on\n",
    "The following steps guide us through the creation of a balanced training data set aiming to train a RNN model capable of highlight pixels of a USGS spectral library class entitled *Schwertmannite BZ93-1 s06av95a=b*. If everything goes well, two shapefiles (and its aggregates) shall be created: **points_schwert.shp** and **points_nonschwert.shp**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Create a grid of regular points\n",
    "\n",
    "We are going to create a shapefile of type *Point* with a grid of equally spaced points, each placed right in the center of each image's pixel. This is going to be accomplished using the \"Regular Points\" algorithm of QGIS. If you are using QGIS 3.2.1, after importing the aforementioned classified image (*ang20150420t182050_corr_v1e_img_class_clipped.img*) to your current project, navigate through menus *Vector* > *Research Tools* > *Regular Points...*. Once opened, at the *Input extent* field, click on the ellipsis button and choose *Use layer/canvas extent*. You will be promped to pick one of the loaded layers of yor project; choose the option that refer to the previously loaded raster and hit OK. Now you'll have to input in the field *Point spacing/count* the value of the scene's pixel size: 1.1 (meters). At the field *Initial inset from corner (LH side)*, input half the value of the scene's pixel size: 0.55 (meters). Select the proper *Output layer CRS* option, usually *EPSG:32612 - WGS 84 / UTM zone 12N*. Finnaly, inform the name of the output file: *grid_points.shp* and hit the button *Run in Background*.\n",
    "![Regio of Interest](regular_points.png)\n",
    "\n",
    "**Note: If you don't want or canÂ´t perform this step, just go ahead; required files were already created.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Extract raster values into points\n",
    "Now, we have to extract the values from the pixels of the rasters *ang20150420t182050_corr_v1e_img_class_clipped* and *ang20150420t182050_corr_v1e_img_score_clipped* into the points created in previous step. To do that, we are going to use a QGIS Plugin called *Point sampling tool*. You can install it by navigating through menus *Plugins* > *Manage and Install Plugins...*; typing \"Point sampling tool\" in the *Search* field and clicking *Install Plugin*. Before opening the plugin, make sure to import the two aforementioned rasters and make theirs layers visible. You can now open the Plugin in *Plugins* > *Analyses* > *Point sampling tool*. Once opened, select *grid_points* at the *Layer containing sampling points* menu. In the field *Layers with fields/bands to get values from:*, select the two rasters. At the same tab, input the path where to save the output shapefile in the *Output point vector layer*: grid_classified.shp. Now, go to the second tab *Fields* and set the following field names to each of the rasters: *clsUSGSv6* for the class raster; and *SAMscore* for the scores raster.\n",
    "\n",
    "![Regio of Interest](field_names.png)\n",
    "\n",
    "**Note: for your convinience, files created through this step are already present in the directory:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls grid_classified.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: List of present classes\n",
    "This script creates a list of all the USGS spectral library (v6) classes ids spotted in the classified raster and stores them into a file called classes.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sys\n",
    "from osgeo import gdal\n",
    "\n",
    "ds = gdal.Open(\"data/ang20150420t182050_corr_v1e_img_class_clipped.img\")\n",
    "\n",
    "existence = numpy.full(1366, False, dtype=bool)\n",
    "for i in range(0,ds.RasterXSize):\n",
    "\tfor j in range(0,ds.RasterYSize):\n",
    "\t\tvalue = ds.ReadAsArray(xoff=i, yoff=j, xsize=1, ysize=1)\n",
    "\t\texistence[value]=True\n",
    "\n",
    "# ignore nodata and Schwertmannite pixels\n",
    "existence[0]=False\n",
    "existence[742]=False\n",
    "\n",
    "try:\n",
    "    exist_classes_file = open(\"classes.txt\",\"w\")\n",
    "    for cls,val in enumerate(existence):\n",
    "        if val == True:\n",
    "            exist_classes_file.write(\"{}\\n\".format(cls))\n",
    "    exist_classes_file.close()\n",
    "    print(\"Outcome: classes.txt successfully created.\")\n",
    "except:\n",
    "    print(\"Something went wrong while creating file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Prune low-scored pixels\n",
    "Sort the points by SAM's confidence value in descending order and get only the top 100 of each class listed in classes.txt, saving them into separate files inside *selected_points* directory. (GDAL 2.1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory = \"./selected_points\"\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory) \n",
    "!while read -r line; do ogr2ogr -overwrite \\\n",
    "-sql \"SELECT SAMscore,geometry FROM grid_classified WHERE clsUSGSv6=$line ORDER BY SAMscore DESC LIMIT 100\" \\\n",
    "-dialect \"sqlite\" selected_points/$line.shp grid_classified.shp; done < classes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When above command has been completed, a shapefile of each class should reside within the *selected_points* directory. Note that not all classes have a total of 100 points, as there's many classes of which don't sum up enough points in the whole scene, which led to a certain degree of unbalancing between subclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls selected_points/*.shp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: non-Schwertmannite points\n",
    "Merge the non-Schwertmannite points into a single file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!for file in selected_points/*.shp; do ogr2ogr -update -append points_nonschwert.shp $file -f \"ESRI Shapefile\"; done;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the above command has been completed, the shapefile *points_nonschwert.shp* with 5878 (the sum of all subclasses) non-Schwertmannite points should be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls points_nonschwert.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Schwertmannite points\n",
    "Now, it's time to create the Schwertmannite (class id 742) file. This is done by issuing the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ogr2ogr -overwrite \\\n",
    "-sql \"SELECT SAMscore,geometry FROM grid_classified WHERE clsUSGSv6=742 ORDER BY SAMscore DESC LIMIT 5878\" \\\n",
    "-dialect \"sqlite\" points_schwert.shp grid_classified.shp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the above command has been completed, the shapefile *points_schwert.shp* with all its 5878 Schwertmannite points should be created. This amount was choosen in order to keep a balanced data set between target and non-target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls points_schwert.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Visualization\n",
    "\n",
    "Below, an image in which red stars mean points of **points_schwert.shp** while circles refer to points of **points_nonschwert.shp**, with different colors representing different USGS spectral library classes:\n",
    "![Regio of Interest](samples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the model\n",
    "With the training points in hand, we must now input them into a training algorithm in order to build our model. We are going to acomplish this through [this piece of code](https://github.com/evandroct/dl-generalize). Please, get it by cloning it's repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/evandroct/dl-generalize modelfactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing [installation steps](https://github.com/evandroct/dl-generalize#installation), we have to move the training samples files we just created, overwriting the ones dowloaded along with the repository in order to update them into the training algorithm directory tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp points_* modelfactory/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fetch a tar.gz file containing the original AVIRIS-NG scene that derived the RGB, classified and scored rasters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.lapig.iesa.ufg.br/drive/index.php/s/IWebcJhmreFeZYP/download -O modelfactory/data/example_scene.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract it's content and place them into default path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf  modelfactory/data/example_scene.tar.gz -C modelfactory/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well, we are good to go. Cast the following command in order to start the training process (this could take a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 modelfactory/run.py train modelfactory/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the 200 epochs is done, we can run another command to see which models lead us to the best accuracy (or loss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 modelfactory/run.py evaluate modelfactory/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to pick one of the three created models:\n",
    "- modelfactory/data/model_last.h5 - the model with the last epoch parameters (after 200 epochs)\n",
    "- modelfactory/data/model_acc.h5 - the model with parameters correponding to the best accuracy epoch\n",
    "- modelfactory/data/model_loss.h5 - the model with parameters correponding to the best loss epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then copy one of the models to desired path and use it as value to the *model_file_name* argument of the [callback function responsible for performing the RNN Pycoal's classification](https://github.com/capstone-coal/pycoal/pull/159/files/026a92dd1e62d8b95e4186c4053eb65a2a4f67be#diff-cec2695d4fcca9629ece7cb3be83e5bfR156)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
